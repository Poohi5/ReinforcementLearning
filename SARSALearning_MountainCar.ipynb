{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARSALearning_MountainCar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfdtWUpNlKk3"
      },
      "source": [
        "#### Install gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDb8o8FgibBt",
        "outputId": "820f18b3-bd0b-424f-fd22-f160acf5232b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFUB64oalNEX"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6uDW74-ifz0"
      },
      "source": [
        "import gym\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vb4zY95lG58"
      },
      "source": [
        "#### Creating gym environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNB4ie_9lCS-"
      },
      "source": [
        "env = gym.make('MountainCar-v0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RURF0ZuLpFH0"
      },
      "source": [
        "#### Assign hyperparameters, no. of states and episodes, learning rate, discount factor gamma "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpr8akeAljHp"
      },
      "source": [
        "n_states = 40 \n",
        "episodes = 10 \n",
        "\n",
        "initial_lr = 1.0 \n",
        "min_lr = 0.005 \n",
        "gamma = 0.99 \n",
        "max_steps = 300\n",
        "epsilon = 0.05\n",
        "\n",
        "env = env.unwrapped\n",
        "env.seed(0)    \n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZmEwccvpllV"
      },
      "source": [
        "#### Define a function which will perform discretization of the continuous state space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3erF2wgpgeh"
      },
      "source": [
        "def discretization(env, obs):\n",
        "    \n",
        "    env_low = env.observation_space.low\n",
        "    env_high = env.observation_space.high\n",
        "    \n",
        "    env_den = (env_high - env_low) / n_states\n",
        "    pos_den = env_den[0]\n",
        "    vel_den = env_den[1]\n",
        "    \n",
        "    pos_high = env_high[0]\n",
        "    pos_low = env_low[0]\n",
        "    vel_high = env_high[1]\n",
        "    vel_low = env_low[1]\n",
        "    \n",
        "    pos_scaled = int((obs[0] - pos_low)/pos_den)  #converts to an integer value\n",
        "    vel_scaled = int((obs[1] - vel_low)/vel_den)  #converts to an integer value\n",
        "    \n",
        "    return pos_scaled,vel_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BImjlUbSp3_t"
      },
      "source": [
        "#### Initialize Q table and update Q-values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_XpM6jMrDa1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67LrVYuJqDC5"
      },
      "source": [
        "#Q table\n",
        "#rows are states but here state is 2-D pos,vel\n",
        "#columns are actions\n",
        "#therefore, Q- table would be 3-D\n",
        "\n",
        "q_table = np.zeros((n_states,n_states,env.action_space.n))\n",
        "total_steps = 0\n",
        "for episode in range(episodes):\n",
        "   obs = env.reset()\n",
        "   total_reward = 0\n",
        "   # decreasing learning rate alpha over time\n",
        "   alpha = max(min_lr,initial_lr*(gamma**(episode//100)))\n",
        "   steps = 0\n",
        "\n",
        "   #action for the initial state using epsilon greedy\n",
        "   if np.random.uniform(low=0,high=1) < epsilon:\n",
        "        a = np.random.choice(env.action_space.n)\n",
        "   else:\n",
        "        pos,vel = discretization(env,obs)\n",
        "        a = np.argmax(q_table[pos][vel])\n",
        "  \n",
        "   while True:\n",
        "      env.render()\n",
        "      pos,vel = discretization(env,obs)\n",
        "    \n",
        "      obs,reward,terminate,_ = env.step(a) \n",
        "      total_reward += abs(obs[0]+0.5) \n",
        "      pos_,vel_ = discretization(env,obs)\n",
        "\n",
        "      #action for the next state using epsilon greedy\n",
        "      if np.random.uniform(low=0,high=1) < epsilon:\n",
        "          a_ = np.random.choice(env.action_space.n)\n",
        "      else:\n",
        "          a_ = np.argmax(q_table[pos_][vel_])\n",
        "\n",
        "      #q-table update\n",
        "      q_table[pos][vel][a] = (1-alpha)*q_table[pos][vel][a] + alpha*(reward+gamma*q_table[pos_][vel_][a_])\n",
        "      steps+=1\n",
        "      if terminate:\n",
        "          break\n",
        "      a = a_\n",
        "   print(\"Episode {} completed with total reward {} in {} steps\".format(episode+1,total_reward,steps)) \n",
        "while True: #to hold the render at the last step when Car passes the flag\n",
        "   env.render()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}